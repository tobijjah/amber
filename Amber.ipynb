{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": "true"
   },
   "source": [
    " # Table of Contents\n",
    "<div class=\"toc\" style=\"margin-top: 1em;\"><ul class=\"toc-item\" id=\"toc-level0\"><li><span><a href=\"http://localhost:8888/notebooks/Amber.ipynb#A-case-study-on-tree-cover-change-from-2001-till-2013-driven-by-illegal-amber-mining-in-the-upper-north-oblast-Rivne/Ukraine\" data-toc-modified-id=\"A-case-study-on-tree-cover-change-from-2001-till-2013-driven-by-illegal-amber-mining-in-the-upper-north-oblast-Rivne/Ukraine-1\" data-vivaldi-spatnav-clickable=\"1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>A case study on tree cover change from 2001 till 2013 driven by illegal amber mining in the upper north oblast Rivne/Ukraine</a></span><ul class=\"toc-item\"><li><span><a href=\"http://localhost:8888/notebooks/Amber.ipynb#Introduction\" data-toc-modified-id=\"Introduction-1.1\" data-vivaldi-spatnav-clickable=\"1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Introduction</a></span></li><li><span><a href=\"http://localhost:8888/notebooks/Amber.ipynb#Data-and-methods\" data-toc-modified-id=\"Data-and-methods-1.2\" data-vivaldi-spatnav-clickable=\"1\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Data and methods</a></span><ul class=\"toc-item\"><li><span><a href=\"http://localhost:8888/notebooks/Amber.ipynb#Study-area-and-data\" data-toc-modified-id=\"Study-area-and-data-1.2.1\" data-vivaldi-spatnav-clickable=\"1\"><span class=\"toc-item-num\">1.2.1&nbsp;&nbsp;</span>Study area and data</a></span></li><li><span><a href=\"http://localhost:8888/notebooks/Amber.ipynb#Methods\" data-toc-modified-id=\"Methods-1.2.2\" data-vivaldi-spatnav-clickable=\"1\"><span class=\"toc-item-num\">1.2.2&nbsp;&nbsp;</span>Methods</a></span><ul class=\"toc-item\"><li><span><a href=\"http://localhost:8888/notebooks/Amber.ipynb#Software-and-libraries\" data-toc-modified-id=\"Software-and-libraries-1.2.2.1\" data-vivaldi-spatnav-clickable=\"1\"><span class=\"toc-item-num\">1.2.2.1&nbsp;&nbsp;</span>Software and libraries</a></span></li><li><span><a href=\"http://localhost:8888/notebooks/Amber.ipynb#Tree-cover-reference-layer\" data-toc-modified-id=\"Tree-cover-reference-layer-1.2.2.2\" data-vivaldi-spatnav-clickable=\"1\"><span class=\"toc-item-num\">1.2.2.2&nbsp;&nbsp;</span>Tree cover reference layer</a></span></li><li><span><a href=\"http://localhost:8888/notebooks/Amber.ipynb#Detection-of-amber-mining-sites\" data-toc-modified-id=\"Detection-of-amber-mining-sites-1.2.2.3\" data-vivaldi-spatnav-clickable=\"1\"><span class=\"toc-item-num\">1.2.2.3&nbsp;&nbsp;</span>Detection of amber mining sites</a></span></li></ul></li></ul></li><li><span><a href=\"http://localhost:8888/notebooks/Amber.ipynb#Results-and-discussion\" data-toc-modified-id=\"Results-and-discussion-1.3\" data-vivaldi-spatnav-clickable=\"1\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Results and discussion</a></span><ul class=\"toc-item\"><ul class=\"toc-item\"><li><span><a href=\"http://localhost:8888/notebooks/Amber.ipynb#Tree-cover-2000\" data-toc-modified-id=\"Tree-cover-2000-1.3.0.1\" data-vivaldi-spatnav-clickable=\"1\"><span class=\"toc-item-num\">1.3.0.1&nbsp;&nbsp;</span>Tree cover 2000</a></span></li><li><span><a href=\"http://localhost:8888/notebooks/Amber.ipynb#Amber-mining-sites\" data-toc-modified-id=\"Amber-mining-sites-1.3.0.2\" data-vivaldi-spatnav-clickable=\"1\"><span class=\"toc-item-num\">1.3.0.2&nbsp;&nbsp;</span>Amber mining sites</a></span></li></ul></ul></li></ul></li><li><span><a href=\"http://localhost:8888/notebooks/Amber.ipynb#References\" data-toc-modified-id=\"References-2\" data-vivaldi-spatnav-clickable=\"1\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>References</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A case study on tree cover change from 2001 till 2013 driven by illegal amber mining in the upper north oblast Rivne/Ukraine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Anthropogen driven land cover change is one of the major causes for tree cover respectively forest loss. For science it is an important task to determine the extent and spatial distribution of this land cover changes to gather knowledge for the development of policy frameworks for prevention and resilience. Particularly, the 1.5 degrees goal and the major role of forest as carbon storage should strengthen the research in such topics because the ongoing degradation of tree cover releases a vast amount of carbon through biomass loss and soil degradation.  \n",
    "According to Piechal Ukraine has the worlds second largest deposits of amber \\cite{Piechal2017}. A vast amount of this amber deposits is located in the western part of the country, particularly in north-western areas of Volyn, Rivne and Zhytomyr Oblasts as **Figure 1** shows. Overall, the oblast Rivne is known for the biggest deposits of the most valuable grades of amber which can be exploited \\cite{Hnatushenko2017}. The technological easy and cheap exploitation of amber deposits in soil layer depths between 2 and 10 meters puts a large momentum on this mining disturbances. Mostly, the organic mineral is exhausted with a approach called hydro mining. This method uses a water pump and fire hose to press water through a long metal pipe into deeper soil layers. The water pressure turns the soil into a liquid suspension and amber floats up and can be gathered on the soil surface. After, extraction deep pits and a heavily degraded soil layer structure remains which is difficult to recultivate, **Figure 2**. Estimates suppose that approximately 150-200 tonnes of amber are illegally exploited per year at market value of US$ 200-300 million, whereas only approximately 4 tonnes are legally mined.  \n",
    "Piechal references governmental sources which claims that around 6,000 hectars of forest have been destroyed due to illegal mining. Further, he mentions that probably more than 10,000 hectars of forest areas have been affected. Hnatushenko et al. estimates suggest  an area of 500 hectars is disturbed by illegal mining in the north Rivne region (Vladimirets district) between 2002 and 2015. \n",
    "\n",
    "This study has the following research goals:\n",
    "- detection of amber mining sites within tree covered area\n",
    "- estimating the amount of disturbed forest area\n",
    "- providing yearly estimates at a temporal solution of 13 years\n",
    "\n",
    "![Deposit](img/amber_deposit_t_piechal_amber_rush_in_ukraine.png)\n",
    "**Figure 1:** Main amber deposits in Ukraine \\cite{Piechal2017}.\n",
    "![Site](img/amber_mining_site_national_geographic.png)\n",
    "**Figure 2:** Impact of illegal amber mining on the local environment \\cite{Hoffman2017}."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data and methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Study area and data\n",
    "The study area is located in the far north of the oblast Rivne in the Ukraine, [**Figure 3**](img/locator.png). This area is characterized by a moderate continental climate with cold, snowy winters and warm summers. The entire study area covering an extent of 10582.33 km$^2$ between the coordinate pairs 51.8N, 25.6E and 51.0N, 27.6E (left top corner, right bottom corner, WGS84). As mentioned in the introduction the selected study area should be affected by amber mining disturbances and this area is know for severe occurrences of this impacts.  \n",
    "To accomplish the temporal aspect of this study 14 satellite images must be selected from the corresponding years 2000 till 2013. Furthermore, the images should meet the following criteria: acquisition date in vegetation period, no or low cloud cover, all images as far as possible from the same time period and the same spatial region. The selection of an appropriate dataset was conducted with the EarthExplorer online service provided by the USGS (United States Geological Survey). With means by this service a set of 8 different Landsat 7 satellite images was selected listed in **Table 1**. These images covering the time periods 2000 till 2002, 2005, 2007-2008, 2010 and 2013. Unfortunately, no feasible Landsat 7 images where available for the remaining time periods, all of them had a cloud cover over 50 %.\n",
    "\n",
    "ID | Acquisition date\n",
    ":--- | ---:\n",
    "LE07_L1TP_184024_20000612_20170211_01_T1 | 06-12-2000\n",
    "LE07_L1TP_184024_20010717_20170204_01_T1 | 07-17-2001 \n",
    "LE07_L1TP_184024_20020704_20170129_01_T1 | 07-04-2002 \n",
    "LE07_L1TP_184024_20050712_20170113_01_T1 | 07-12-2005 \n",
    "LE07_L1TP_184024_20070718_20170103_01_T1 | 07-18-2007\n",
    "LE07_L1TP_184024_20080602_20161229_01_T1 | 06-02-2008\n",
    "LE07_L1TP_184024_20100523_20161214_01_T1 | 05-23-2010 \n",
    "LE07_L1TP_184024_20130819_20161122_01_T1 | 08-19-2013\n",
    "\n",
    "**Table 1:** Landsat 7 images used for this study.\n",
    "\n",
    "![StudyArea](img/locator.png)\n",
    "**Figure 3:** The top image shows a locator map of the study area and the bottom image is a RGB-Channel Landsat 7 image of the study area from 2000."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Methods\n",
    "#### Software and libraries\n",
    "The entire processing pipeline relies only on open source software and the code to accomplish the several steps of the processing pipeline is published on [GitHub](https://github.com/tobijjah/amber).  \n",
    "The programming language Python drives all the geo-processing operations. For handling GIS data, like raster (GeoTIFF) and vector files (ESRI-Shapefile) rasterio as well as geopandas are used. Both libraries provide a convenient access and basic operations on GIS datasets. Mostly, the API of this modules is accessed through some small custom made [wrapper functions](src/utils.py) which are comprehensively documented. Numpy and pandas are used to handle large sets of numeric values and to apply value-wise operations on them. Sklearn provides a comprehensive API for supervised and unsupervised machine learning algorithms. This study uses a Random Forest classifier and a Linear Discriminant Analysis. Finally, Bokeh provides a rich interface for plotting interactive graphs. Additionally, QGIS 2.14 is used for visual interpretation of Landsat scenes and map composition.  \n",
    "This study provides all required code to successfully execute the entire processing pipeline. Just clone the repository to your local client and install all required requirements with `pip install -r requirements.txt`. Please, consider the code is tested on Python3.5, hence I provide no warranty differing Python versions. Moreover, the processing pipeline is memory intensive, your local machine should provide on minimum 16GB RAM. Also, you must download the required Landsat scene archives from **Table 1**. Please move, this archives to the data/arch folder of the project and execute the code cells in this notebook in a top-down manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bk-root\">\n",
       "        <a href=\"https://bokeh.pydata.org\" target=\"_blank\" class=\"bk-logo bk-logo-small bk-logo-notebook\"></a>\n",
       "        <span id=\"7414c6ce-2d49-4a51-a498-cdfc1b8635d3\">Loading BokehJS ...</span>\n",
       "    </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "(function(root) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  var force = true;\n",
       "\n",
       "  if (typeof (root._bokeh_onload_callbacks) === \"undefined\" || force === true) {\n",
       "    root._bokeh_onload_callbacks = [];\n",
       "    root._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "  var JS_MIME_TYPE = 'application/javascript';\n",
       "  var HTML_MIME_TYPE = 'text/html';\n",
       "  var EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n",
       "  var CLASS_NAME = 'output_bokeh rendered_html';\n",
       "\n",
       "  /**\n",
       "   * Render data to the DOM node\n",
       "   */\n",
       "  function render(props, node) {\n",
       "    var script = document.createElement(\"script\");\n",
       "    node.appendChild(script);\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when an output is cleared or removed\n",
       "   */\n",
       "  function handleClearOutput(event, handle) {\n",
       "    var cell = handle.cell;\n",
       "\n",
       "    var id = cell.output_area._bokeh_element_id;\n",
       "    var server_id = cell.output_area._bokeh_server_id;\n",
       "    // Clean up Bokeh references\n",
       "    if (id !== undefined) {\n",
       "      Bokeh.index[id].model.document.clear();\n",
       "      delete Bokeh.index[id];\n",
       "    }\n",
       "\n",
       "    if (server_id !== undefined) {\n",
       "      // Clean up Bokeh references\n",
       "      var cmd = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n",
       "      cell.notebook.kernel.execute(cmd, {\n",
       "        iopub: {\n",
       "          output: function(msg) {\n",
       "            var element_id = msg.content.text.trim();\n",
       "            Bokeh.index[element_id].model.document.clear();\n",
       "            delete Bokeh.index[element_id];\n",
       "          }\n",
       "        }\n",
       "      });\n",
       "      // Destroy server and session\n",
       "      var cmd = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n",
       "      cell.notebook.kernel.execute(cmd);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when a new output is added\n",
       "   */\n",
       "  function handleAddOutput(event, handle) {\n",
       "    var output_area = handle.output_area;\n",
       "    var output = handle.output;\n",
       "\n",
       "    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n",
       "    if ((output.output_type != \"display_data\") || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n",
       "      return\n",
       "    }\n",
       "\n",
       "    var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n",
       "\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n",
       "      toinsert[0].firstChild.textContent = output.data[JS_MIME_TYPE];\n",
       "      // store reference to embed id on output_area\n",
       "      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n",
       "    }\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n",
       "      var bk_div = document.createElement(\"div\");\n",
       "      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n",
       "      var script_attrs = bk_div.children[0].attributes;\n",
       "      for (var i = 0; i < script_attrs.length; i++) {\n",
       "        toinsert[0].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n",
       "      }\n",
       "      // store reference to server id on output_area\n",
       "      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function register_renderer(events, OutputArea) {\n",
       "\n",
       "    function append_mime(data, metadata, element) {\n",
       "      // create a DOM node to render to\n",
       "      var toinsert = this.create_output_subarea(\n",
       "        metadata,\n",
       "        CLASS_NAME,\n",
       "        EXEC_MIME_TYPE\n",
       "      );\n",
       "      this.keyboard_manager.register_events(toinsert);\n",
       "      // Render to node\n",
       "      var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n",
       "      render(props, toinsert[0]);\n",
       "      element.append(toinsert);\n",
       "      return toinsert\n",
       "    }\n",
       "\n",
       "    /* Handle when an output is cleared or removed */\n",
       "    events.on('clear_output.CodeCell', handleClearOutput);\n",
       "    events.on('delete.Cell', handleClearOutput);\n",
       "\n",
       "    /* Handle when a new output is added */\n",
       "    events.on('output_added.OutputArea', handleAddOutput);\n",
       "\n",
       "    /**\n",
       "     * Register the mime type and append_mime function with output_area\n",
       "     */\n",
       "    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n",
       "      /* Is output safe? */\n",
       "      safe: true,\n",
       "      /* Index of renderer in `output_area.display_order` */\n",
       "      index: 0\n",
       "    });\n",
       "  }\n",
       "\n",
       "  // register the mime type if in Jupyter Notebook environment and previously unregistered\n",
       "  if (root.Jupyter !== undefined) {\n",
       "    var events = require('base/js/events');\n",
       "    var OutputArea = require('notebook/js/outputarea').OutputArea;\n",
       "\n",
       "    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n",
       "      register_renderer(events, OutputArea);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  \n",
       "  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n",
       "    root._bokeh_timeout = Date.now() + 5000;\n",
       "    root._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  var NB_LOAD_WARNING = {'data': {'text/html':\n",
       "     \"<div style='background-color: #fdd'>\\n\"+\n",
       "     \"<p>\\n\"+\n",
       "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "     \"</p>\\n\"+\n",
       "     \"<ul>\\n\"+\n",
       "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "     \"</ul>\\n\"+\n",
       "     \"<code>\\n\"+\n",
       "     \"from bokeh.resources import INLINE\\n\"+\n",
       "     \"output_notebook(resources=INLINE)\\n\"+\n",
       "     \"</code>\\n\"+\n",
       "     \"</div>\"}};\n",
       "\n",
       "  function display_loaded() {\n",
       "    var el = document.getElementById(\"7414c6ce-2d49-4a51-a498-cdfc1b8635d3\");\n",
       "    if (el != null) {\n",
       "      el.textContent = \"BokehJS is loading...\";\n",
       "    }\n",
       "    if (root.Bokeh !== undefined) {\n",
       "      if (el != null) {\n",
       "        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n",
       "      }\n",
       "    } else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(display_loaded, 100)\n",
       "    }\n",
       "  }\n",
       "\n",
       "\n",
       "  function run_callbacks() {\n",
       "    try {\n",
       "      root._bokeh_onload_callbacks.forEach(function(callback) { callback() });\n",
       "    }\n",
       "    finally {\n",
       "      delete root._bokeh_onload_callbacks\n",
       "    }\n",
       "    console.info(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(js_urls, callback) {\n",
       "    root._bokeh_onload_callbacks.push(callback);\n",
       "    if (root._bokeh_is_loading > 0) {\n",
       "      console.log(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.log(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    root._bokeh_is_loading = js_urls.length;\n",
       "    for (var i = 0; i < js_urls.length; i++) {\n",
       "      var url = js_urls[i];\n",
       "      var s = document.createElement('script');\n",
       "      s.src = url;\n",
       "      s.async = false;\n",
       "      s.onreadystatechange = s.onload = function() {\n",
       "        root._bokeh_is_loading--;\n",
       "        if (root._bokeh_is_loading === 0) {\n",
       "          console.log(\"Bokeh: all BokehJS libraries loaded\");\n",
       "          run_callbacks()\n",
       "        }\n",
       "      };\n",
       "      s.onerror = function() {\n",
       "        console.warn(\"failed to load library \" + url);\n",
       "      };\n",
       "      console.log(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "    }\n",
       "  };var element = document.getElementById(\"7414c6ce-2d49-4a51-a498-cdfc1b8635d3\");\n",
       "  if (element == null) {\n",
       "    console.log(\"Bokeh: ERROR: autoload.js configured with elementid '7414c6ce-2d49-4a51-a498-cdfc1b8635d3' but no matching script tag was found. \")\n",
       "    return false;\n",
       "  }\n",
       "\n",
       "  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-0.12.13.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.13.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-0.12.13.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-gl-0.12.13.min.js\"];\n",
       "\n",
       "  var inline_js = [\n",
       "    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "    \n",
       "    function(Bokeh) {\n",
       "      \n",
       "    },\n",
       "    function(Bokeh) {\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-0.12.13.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-0.12.13.min.css\");\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.13.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.13.min.css\");\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-tables-0.12.13.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-tables-0.12.13.min.css\");\n",
       "    }\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    \n",
       "    if ((root.Bokeh !== undefined) || (force === true)) {\n",
       "      for (var i = 0; i < inline_js.length; i++) {\n",
       "        inline_js[i].call(root, root.Bokeh);\n",
       "      }if (force === true) {\n",
       "        display_loaded();\n",
       "      }} else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!root._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      root._bokeh_failed_load = true;\n",
       "    } else if (force !== true) {\n",
       "      var cell = $(document.getElementById(\"7414c6ce-2d49-4a51-a498-cdfc1b8635d3\")).parents('.cell').data().cell;\n",
       "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "    }\n",
       "\n",
       "  }\n",
       "\n",
       "  if (root._bokeh_is_loading === 0) {\n",
       "    console.log(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(js_urls, function() {\n",
       "      console.log(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(window));"
      ],
      "application/vnd.bokehjs_load.v0+json": "\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  var force = true;\n\n  if (typeof (root._bokeh_onload_callbacks) === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\n  \n\n  \n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  var NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded() {\n    var el = document.getElementById(\"7414c6ce-2d49-4a51-a498-cdfc1b8635d3\");\n    if (el != null) {\n      el.textContent = \"BokehJS is loading...\";\n    }\n    if (root.Bokeh !== undefined) {\n      if (el != null) {\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(display_loaded, 100)\n    }\n  }\n\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) { callback() });\n    }\n    finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.info(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(js_urls, callback) {\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.log(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.log(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = js_urls.length;\n    for (var i = 0; i < js_urls.length; i++) {\n      var url = js_urls[i];\n      var s = document.createElement('script');\n      s.src = url;\n      s.async = false;\n      s.onreadystatechange = s.onload = function() {\n        root._bokeh_is_loading--;\n        if (root._bokeh_is_loading === 0) {\n          console.log(\"Bokeh: all BokehJS libraries loaded\");\n          run_callbacks()\n        }\n      };\n      s.onerror = function() {\n        console.warn(\"failed to load library \" + url);\n      };\n      console.log(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.getElementsByTagName(\"head\")[0].appendChild(s);\n    }\n  };var element = document.getElementById(\"7414c6ce-2d49-4a51-a498-cdfc1b8635d3\");\n  if (element == null) {\n    console.log(\"Bokeh: ERROR: autoload.js configured with elementid '7414c6ce-2d49-4a51-a498-cdfc1b8635d3' but no matching script tag was found. \")\n    return false;\n  }\n\n  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-0.12.13.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.13.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-0.12.13.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-gl-0.12.13.min.js\"];\n\n  var inline_js = [\n    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\n    \n    function(Bokeh) {\n      \n    },\n    function(Bokeh) {\n      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-0.12.13.min.css\");\n      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-0.12.13.min.css\");\n      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.13.min.css\");\n      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.13.min.css\");\n      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-tables-0.12.13.min.css\");\n      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-tables-0.12.13.min.css\");\n    }\n  ];\n\n  function run_inline_js() {\n    \n    if ((root.Bokeh !== undefined) || (force === true)) {\n      for (var i = 0; i < inline_js.length; i++) {\n        inline_js[i].call(root, root.Bokeh);\n      }if (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      var cell = $(document.getElementById(\"7414c6ce-2d49-4a51-a498-cdfc1b8635d3\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.log(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(js_urls, function() {\n      console.log(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# third party libs\n",
    "import os  # python stdlib \n",
    "import re  # python stdlib\n",
    "import rasterio  # https://github.com/mapbox/rasterio\n",
    "import numpy as np  # https://github.com/numpy/numpy\n",
    "import pandas as pd  # https://github.com/pandas-dev/pandas\n",
    "import geopandas as gpd  # https://github.com/geopandas/geopandas\n",
    "from pathlib import Path  # python stdlib\n",
    "from collections import namedtuple  # python stdlib\n",
    "from shapely.geometry import Point  # https://github.com/Toblerity/Shapely\n",
    "from sklearn.ensemble import RandomForestClassifier  # http://scikit-learn.org/stable/index.html\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from bokeh.plotting import show, figure, gridplot, output_notebook, ColumnDataSource  # https://github.com/bokeh/bokeh\n",
    "\n",
    "# custom libs\n",
    "from src.landsat import LandsatArchive\n",
    "from src.utils import (get_data_dir,\n",
    "                       ndbi,\n",
    "                       ndvi,\n",
    "                       ltk_cloud_masking,\n",
    "                       RANDOM,\n",
    "                       l7_radiance,\n",
    "                       l7_reflectance,\n",
    "                       write,\n",
    "                       clip_raster,\n",
    "                       reproject_from,\n",
    "                       reproject_like,\n",
    "                       draw_raster_sample,\n",
    "                       confusion_matrix)\n",
    "\n",
    "\n",
    "# make source data folders\n",
    "directories = \"\"\"\n",
    "data\n",
    "data.core\n",
    "data.proc\n",
    "data.prep\n",
    "data.arch\n",
    "\"\"\"\n",
    "\n",
    "# create data directories if not available\n",
    "for item in directories.split():\n",
    "    path = os.sep.join(item.split('.'))\n",
    "    try:\n",
    "        os.mkdir(path)\n",
    "    except OSError:\n",
    "        pass\n",
    "\n",
    "# convenient access to data dir\n",
    "DIRS = get_data_dir(str(Path('data').resolve()))\n",
    "\n",
    "# pyproj definition of WGS84\n",
    "WGS84 = {'init': 'epsg:4326'}\n",
    "\n",
    "# area of interest\n",
    "Bounds = namedtuple('Bounds', 'left bottom right top')\n",
    "AIO = Bounds(25.6, 51.0, 27.6, 51.8)\n",
    "\n",
    "# total number of threads for sklearn\n",
    "THREADS = 4\n",
    "\n",
    "# force bokeh notebook output of plots\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tree cover reference layer\n",
    "Fundamental for detecting annual changes in the forest cover through amber mining disturbances is a reference tree cover layer. Therefore, the oldest raster dataset from 2000 is chosen to compute a reference stratum. As first step the red, green, blue and near infrared (NIR) band of this dataset is converted to top of atmosphere radiance (TOA-Rad) and subsequently to an top of atmosphere reflectance (TOA-Ref) band stack. Next, a normalized difference vegetation index image is computed from the red and nir band. The resulting multi spectral image is re-projected to WGS84 and clipped to the required extent of the study area. Now, a random sample of 500 pixels overall bands is drawn from the multi spectral image and saved as vector file. This sample vector file contains 5 different spectral features (red, green, blue, nir and ndvi) from the atmospherically corrected Landsat scene. These samples were classified with two different labels (tree cover, no tree cover) by visual interpretation. To do so the vector file as well as the multi spectral satellite image were loaded into QGSI, and each sample pixel was classified by their visual class correspondence. Afterwards, a random forest classifier (supervised machine learning) was trained on base of the labeled samples. Approximately 75 % of this samples (random selection) were used as a trainings dataset and the  remaining samples are used for an accuracy assessment. Finally, the trained random forest classifier is used to predict the class labels for the entire Landsat scene. The result of this prediction represents the estimated tree cover at 2000. Now, this reference stratum can be used to determine land cover changes in tree cover for any period after 2000.  \n",
    "The following documented code cells should outline the processing pipeline, if executed they provide the same results as used in this project. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        Spacecraft: LANDSAT_7\n",
      "        Sensor: ETM\n",
      "        Date acquired: 2000-06-12\n",
      "        Cloud cover: 1.0\n",
      "        Quality: 9\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "# load raster dataset from 2000\n",
    "\n",
    "src = DIRS.core / 'l7_2000'\n",
    "\n",
    "# create a LandsatArchive object, for convenient access to raster images\n",
    "if src.is_dir():\n",
    "    # if core directory contains reference scene just read it\n",
    "    l7_2000 = LandsatArchive.read(src)\n",
    "else:\n",
    "    # try to extract reference scene from arch directory\n",
    "    l7_2000 = LandsatArchive.read(DIRS.arch / 'LE07_L1TP_184024_20000612_20170211_01_T1.tar.gz',\n",
    "                                  extract_to=DIRS.core / 'l7_2000')\n",
    "\n",
    "print(l7_2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# compute band metrics and compose band stack\n",
    "\n",
    "# regular expression matching the band index in image name\n",
    "regex = re.compile(r'.*LE07.*_B(\\d)\\.TIF')\n",
    "\n",
    "band_stack = []\n",
    "# iterate over red, green, blue and nir band \n",
    "for name in 'red green blue nir'.split():\n",
    "    # get raster image from dataset\n",
    "    img = l7_2000[name]\n",
    "    # read pixel values as 2 dimensional array\n",
    "    img_data = img.read(1)\n",
    "    # match band index\n",
    "    band_idx = regex.match(img.name).group(1)\n",
    "    \n",
    "    # get required constants from raster dataset metadata\n",
    "    RMIN = l7_2000.metadata.get('MIN_MAX_RADIANCE', 'RADIANCE_MINIMUM_BAND_%s' % band_idx)\n",
    "    RMAX = l7_2000.metadata.get('MIN_MAX_RADIANCE', 'RADIANCE_MAXIMUM_BAND_%s' % band_idx)\n",
    "    QCMIN = l7_2000.metadata.get('MIN_MAX_PIXEL_VALUE', 'QUANTIZE_CAL_MIN_BAND_%s' % band_idx)\n",
    "    QCMAX = l7_2000.metadata.get('MIN_MAX_PIXEL_VALUE', 'QUANTIZE_CAL_MAX_BAND_%s' % band_idx)\n",
    "    ESD = l7_2000.metadata.get('IMAGE_ATTRIBUTES', 'EARTH_SUN_DISTANCE')\n",
    "    SE = l7_2000.metadata.get('IMAGE_ATTRIBUTES', 'SUN_ELEVATION')\n",
    "\n",
    "    # compute top of atmosphere radiance and reflectance\n",
    "    radiance = l7_radiance(img_data, QCMIN, QCMAX, RMIN, RMAX, src_nodata=0)\n",
    "    reflectance = l7_reflectance(radiance, ESD, SE, int(band_idx), src_nodata=0.0)\n",
    "\n",
    "    # save atmospherically corrected band data\n",
    "    band_stack.append(reflectance)\n",
    "    img.close()\n",
    "\n",
    "# compute ndvi with atmospherically corrected red and nir data\n",
    "ndvi_data = ndvi(band_stack[0], band_stack[3])\n",
    "\n",
    "# save ndiv data\n",
    "band_stack.append(ndvi_data)\n",
    "# atmospherically corrected raster image with 5 bands (red, green, blue, nir and ndiv)\n",
    "rgbn_data = np.array(band_stack, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reproject and clip\n",
    "\n",
    "# load coordinate reference system and affine matrix from source raster dataset\n",
    "img = l7_2000['red']\n",
    "crs = img.crs\n",
    "transform = img.transform\n",
    "img.close()\n",
    "\n",
    "# atmospherically corrected raster data to disk\n",
    "rgbn_path = write(rgbn_data, str(DIRS.proc / 'reference_image.tif'), \n",
    "                  driver='GTiff', transform=transform, \n",
    "                  crs=crs)\n",
    "\n",
    "# reproject to WGS84\n",
    "reproject = reproject_from(rgbn_path, WGS84, rgbn_path)\n",
    "\n",
    "# clip image to area of interest\n",
    "clip, transform = clip_raster(reproject, AIO)\n",
    "\n",
    "# write clipped data to disk\n",
    "clip_path = write(clip, reproject, \n",
    "                  driver='GTiff', transform=transform, \n",
    "                  crs=WGS84)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# sample image\n",
    "\n",
    "# open reference image\n",
    "ref_img = rasterio.open(clip_path, 'r')\n",
    "ref_data = ref_img.read()\n",
    "transform = ref_img.transform\n",
    "\n",
    "# draw a random sample from reference image\n",
    "sample = draw_raster_sample(ref_data, affine=transform, samples=500,\n",
    "                            columns=['red', 'green', 'blue', 'nir', 'ndvi'])\n",
    "\n",
    "# convert sample coordinates to a point object\n",
    "points = [Point(coor.x, coor.y) \n",
    "          for idx, coor in sample[['x', 'y']].iterrows()]\n",
    "\n",
    "# create a geopandas data frame with raster sample data \n",
    "geometry = gpd.GeoSeries(points)\n",
    "geo_df = gpd.GeoDataFrame(sample, geometry=geometry)\n",
    "geo_df.crs = WGS84\n",
    "\n",
    "# save raster samples as a vector file\n",
    "geo_df.to_file(str(DIRS.proc / 'samples.shp'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done 442 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=4)]: Done 792 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=4)]: Done 1000 out of 1000 | elapsed:    0.6s finished\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done 442 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done 792 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=4)]: Done 1000 out of 1000 | elapsed:    0.2s finished\n"
     ]
    }
   ],
   "source": [
    "# train random forest\n",
    "\n",
    "# open labeld raster samples\n",
    "samples = gpd.read_file(str(DIRS.prep / 'forest.shp'))\n",
    "# divide samples in training and test dataset \n",
    "samples['is_train'] = RANDOM.uniform(size=len(samples)) <= 0.75\n",
    "train, test = samples[samples.is_train == True], samples[samples.is_train == False]\n",
    "\n",
    "features = train.columns[:5]\n",
    "\n",
    "# create a random forest classifier object\n",
    "clf = RandomForestClassifier(n_jobs=THREADS, random_state=RANDOM, n_estimators=1000, verbose=1)\n",
    "\n",
    "# train the classifier with training samples\n",
    "clf.fit(train[features], train.label)\n",
    "\n",
    "# accuracy assessment with test data\n",
    "assessment = clf.predict(test[features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    8.4s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:   36.0s\n",
      "[Parallel(n_jobs=4)]: Done 442 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=4)]: Done 792 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=4)]: Done 1000 out of 1000 | elapsed:  3.2min finished\n"
     ]
    }
   ],
   "source": [
    "# tree cover classification respectively prediction\n",
    "\n",
    "# reduce dimensionality of reference image \n",
    "px_matrix = ref_data.T.reshape((ref_data.shape[1]*ref_data.shape[2],ref_data.shape[0]))\n",
    "df = pd.DataFrame.from_records(px_matrix, columns=features)\n",
    "\n",
    "# predict label of reference pixels\n",
    "classified = clf.predict(df[features])\n",
    "\n",
    "# reshape predicted labels to corresponding reference image shape\n",
    "classified = np.reshape(classified, (ref_data.shape[2], ref_data.shape[1])).T\n",
    "classified = classified.astype(np.uint8)\n",
    "\n",
    "# save final image on disk\n",
    "treecover_2000 = write(classified, str(DIRS.proc / 'treecover_2000.tif'), crs=WGS84, driver='GTiff',\n",
    "                       transform=transform, compress='lzw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# accuray assessment\n",
    "df = pd.DataFrame.from_records(confusion_matrix(test.label, assessment),\n",
    "                               columns=['Condition positive', 'Condition negative'],\n",
    "                               index=['Prediction positive', 'Prediction negative'])\n",
    "df.to_csv(str(DIRS.proc / 'assess.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Detection of amber mining sites\n",
    "To perform a change detection on the annual tree cover change through mining activity, the appropriate approach would be a collection of ground truth data of real amber mining sites. After, this data can be used to perform on a satellite image a Principal Component Analysis (PCA) to determine the feature or feature combinations which describes the target best. Apparently, it was not possible to visit Rivne for this study, so a pretty rough approach is chosen to determine amber mining sites. The approach uses the following hypothesis: a loss of tree cover is characterized through decreasing NDVI values for a selected pixel, as mentioned in the introduction amber mining activity creates huge areas of bare soil so it should be detectable through an index which has a emphasis on open/bare soil. For this study the selected method is a feature combination of NDVI and Normalized Difference Bareness Index (NDBI). Comparable to the previous section we have a binary decision problem, a pixel within the tree cover reference layer shows a change to bare soil respectively amber mine or not. To solve this problem tree cover a sample set is created with two feature records. This sample set comprises 334 samples of the spectral values of tree cover and supposed amber mines. Amber mines were detected by visual interpretation on three different RGB-Landsat scenes (2000, 2001 and 2002). Finally, the sample set is used to train a Linear Discriminant (LDA) classifier which is applied to detect annual changes in the tree cover respectively a land cover change from forest to amber mine.  \n",
    "To detect these land cover changes in our annual Landsat scenes we have to convert each image to TOA-Ref. Next, we compute the required indexes. Unfortunately, most of the scenes after 2000 had a cloud cover above 10 % so we need a cloud mask for each scene. The cloud mask is computed with the Luo–Trishchenko–Khlopenkov algorithm. Now, we can predict the annual tree cover changes with the LDA classifier. The classifier is applied on the NDVI, NDBI images to predict the corresponding class for each pixel. After, all pixels on the outside of the tree cover reference layer are set to null as well as all pixels within the cloud mask. Finally, we have to consider that all Landsat 7 images acquired after 2003 have a scan line corrector error gap. Therefore, we have to eliminate all pixels with a value of -1, 0 or 1.  \n",
    "The following documented code cells should outline the processing pipeline, if executed they provide the same results as used in this project. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        Spacecraft: LANDSAT_7\n",
      "        Sensor: ETM\n",
      "        Date acquired: 2007-07-18\n",
      "        Cloud cover: 0.0\n",
      "        Quality: 9\n",
      "        \n",
      "\n",
      "        Spacecraft: LANDSAT_7\n",
      "        Sensor: ETM\n",
      "        Date acquired: 2010-05-23\n",
      "        Cloud cover: 10.0\n",
      "        Quality: 9\n",
      "        \n",
      "\n",
      "        Spacecraft: LANDSAT_7\n",
      "        Sensor: ETM\n",
      "        Date acquired: 2013-08-19\n",
      "        Cloud cover: 10.0\n",
      "        Quality: 9\n",
      "        \n",
      "\n",
      "        Spacecraft: LANDSAT_7\n",
      "        Sensor: ETM\n",
      "        Date acquired: 2008-06-02\n",
      "        Cloud cover: 1.0\n",
      "        Quality: 9\n",
      "        \n",
      "\n",
      "        Spacecraft: LANDSAT_7\n",
      "        Sensor: ETM\n",
      "        Date acquired: 2005-07-12\n",
      "        Cloud cover: 19.0\n",
      "        Quality: 9\n",
      "        \n",
      "\n",
      "        Spacecraft: LANDSAT_7\n",
      "        Sensor: ETM\n",
      "        Date acquired: 2002-07-04\n",
      "        Cloud cover: 12.0\n",
      "        Quality: 9\n",
      "        \n",
      "\n",
      "        Spacecraft: LANDSAT_7\n",
      "        Sensor: ETM\n",
      "        Date acquired: 2001-07-17\n",
      "        Cloud cover: 0.0\n",
      "        Quality: 9\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "# load annual landsat scenes\n",
    "\n",
    "if len(os.listdir(str(DIRS.core))) > 1:\n",
    "    # if core contains more than one directory the annual landsat scenes are already extracted\n",
    "    # and we can init their paths\n",
    "    names = os.listdir(str(DIRS.core))\n",
    "    is_archive = False\n",
    "\n",
    "else:\n",
    "    # get annual landsat scene paths from arch directory\n",
    "    names = os.listdir(str(DIRS.arch))\n",
    "    is_archive = True\n",
    "\n",
    "# iterate over annual landsat scene paths and init a LandsatArchive object\n",
    "landsat = []\n",
    "for name in names:\n",
    "    if is_archive:\n",
    "        head = name.split('.')[0]\n",
    "        obj = LandsatArchive.read(DIRS.arch / name, extract_to=DIRS.core / head)\n",
    "    \n",
    "    else:\n",
    "        obj = LandsatArchive.read(DIRS.core / name)\n",
    "    \n",
    "    year = obj.metadata.get('product_metadata', 'date_acquired').split('-')[0]\n",
    "    \n",
    "    # set LandsatArchive alias to the corresponding scene acquisition year\n",
    "    obj.alias = year\n",
    "    \n",
    "    if int(year) > 2000:\n",
    "        landsat.append(obj)\n",
    "        print(obj)\n",
    "\n",
    "# sort objects yearly increasing\n",
    "landsat = sorted(landsat, key=lambda x: x.alias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# compute band metrics normalized difference vegetation index and normalized difference bareness index\n",
    "# create cloud mask\n",
    "\n",
    "# regular expression matching the band index in image name\n",
    "regex = re.compile(r'.*LE07.*_B(\\d)\\.TIF')\n",
    "\n",
    "images = []\n",
    "# iterate over annual landsat scenes\n",
    "for data in landsat:\n",
    "    # store metadata of current landsat scene\n",
    "    img = data['red']\n",
    "    crs = img.crs\n",
    "    transform = img.transform\n",
    "    img.close()\n",
    "    \n",
    "    band_stack = []\n",
    "    # apply band metrics to red, blue, nir, and swir2 band\n",
    "    for name in 'red blue nir swir2'.split():\n",
    "        # get current band from landsat scene\n",
    "        img = data[name]\n",
    "        # read image data as 2D array\n",
    "        img_data = img.read(1)\n",
    "        # match band index\n",
    "        band_idx = regex.match(img.name).group(1)\n",
    "\n",
    "        # get metadata constants for TOA-Rad and TOA-Ref computation\n",
    "        RMIN = data.metadata.get('MIN_MAX_RADIANCE', 'RADIANCE_MINIMUM_BAND_%s' % band_idx)\n",
    "        RMAX = data.metadata.get('MIN_MAX_RADIANCE', 'RADIANCE_MAXIMUM_BAND_%s' % band_idx)\n",
    "        QCMIN = data.metadata.get('MIN_MAX_PIXEL_VALUE', 'QUANTIZE_CAL_MIN_BAND_%s' % band_idx)\n",
    "        QCMAX = data.metadata.get('MIN_MAX_PIXEL_VALUE', 'QUANTIZE_CAL_MAX_BAND_%s' % band_idx)\n",
    "        ESD = data.metadata.get('IMAGE_ATTRIBUTES', 'EARTH_SUN_DISTANCE')\n",
    "        SE = data.metadata.get('IMAGE_ATTRIBUTES', 'SUN_ELEVATION')\n",
    "\n",
    "        # compute TOA-Rad and TOA-Ref\n",
    "        radiance = l7_radiance(img_data, QCMIN, QCMAX, RMIN, RMAX, src_nodata=0)\n",
    "        reflectance = l7_reflectance(radiance, ESD, SE, int(band_idx), src_nodata=0.0)\n",
    "\n",
    "        # save atmospherically corrected band data\n",
    "        band_stack.append(reflectance)\n",
    "        img.close()\n",
    "\n",
    "    # compute ndvi and ndbi\n",
    "    ndvi_data = ndvi(band_stack[0], band_stack[2])\n",
    "    ndbi_data = ndbi(band_stack[3], band_stack[0], band_stack[2], band_stack[1])\n",
    "\n",
    "    # final band stack\n",
    "    rgbn_data = np.array([ndvi_data, ndbi_data], dtype=np.float32)\n",
    "    \n",
    "    # compute cloud mask for current landsat scene\n",
    "    cloud_mask = ltk_cloud_masking(*band_stack[:3])\n",
    "\n",
    "    # save final band stack and cloud mask on disk\n",
    "    for idx, product in enumerate((rgbn_data, cloud_mask)):\n",
    "        name = '{}_{}.tif'.format(idx, data.alias)\n",
    "        \n",
    "        # write current img data to disk\n",
    "        full_size = write(product, str(DIRS.proc / name), \n",
    "                          driver='GTiff', transform=transform, \n",
    "                          crs=crs)\n",
    "        reproject = reproject_like(str(DIRS.proc / 'reference_image.tif'), full_size, full_size)\n",
    "        \n",
    "        clip, affine = clip_raster(reproject, AIO)\n",
    "        \n",
    "        clip_path = write(clip, reproject, \n",
    "                          driver='GTiff', transform=affine, \n",
    "                          crs=WGS84)\n",
    "        \n",
    "        # save reference path to ndvi, ndbi and cloud image\n",
    "        images.append(clip_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearDiscriminantAnalysis(n_components=None, priors=None, shrinkage=None,\n",
       "              solver='lsqr', store_covariance=False, tol=0.0001)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train model linear discriminant analysis\n",
    "\n",
    "# get supposed sample amber mining sites\n",
    "samples = gpd.read_file(str(DIRS.prep / 'mines.shp'))\n",
    "\n",
    "features = samples.columns[:2]\n",
    "\n",
    "# prepare a linear discriminant classifier\n",
    "lda = LinearDiscriminantAnalysis(solver='lsqr')\n",
    "\n",
    "# train lda with the entire sample dataset\n",
    "lda.fit(samples[features], samples.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# mining site prediction\n",
    "\n",
    "# open and read tree cover reference stratum\n",
    "treecover = rasterio.open(treecover_2000, 'r')\n",
    "transform = treecover.transform\n",
    "cover = treecover.read(1)\n",
    "\n",
    "# iterate over annual ndvi, ndbi and corresponding cloud mask\n",
    "for idx, values in enumerate(zip(images[::2], images[1::2])):\n",
    "    image, cloud = values\n",
    "    \n",
    "    # open and read ndvi, ndbi scene\n",
    "    img = rasterio.open(image)\n",
    "    to_classify = img.read()\n",
    "\n",
    "    # open and read cloud mask\n",
    "    cloud_cover = rasterio.open(cloud)\n",
    "    c = cloud_cover.read(1)\n",
    " \n",
    "    # reduce dimensionalty of ndvi, ndbi image data\n",
    "    px_matrix = to_classify.T.reshape((to_classify.shape[1]*to_classify.shape[2],to_classify.shape[0]))\n",
    "    df = pd.DataFrame.from_records(px_matrix, columns=features)\n",
    "    \n",
    "    # predict class \n",
    "    classified = lda.predict(df[features])\n",
    "\n",
    "    # reshape prediction to source dimension\n",
    "    classified = np.reshape(classified, (to_classify.shape[2], to_classify.shape[1])).T\n",
    "    classified = classified.astype(np.uint8)\n",
    "\n",
    "    # set pixels outlying of the refrence tree cover to zero\n",
    "    classified[cover == 0] = 0\n",
    "    \n",
    "    # set scan line corrector gap pixels to zero\n",
    "    classified[to_classify[0] == 0.0] = 0\n",
    "    classified[to_classify[0] == 1.0] = 0\n",
    "    classified[to_classify[0] == -1.0] = 0\n",
    "    classified[to_classify[1] == 0.0] = 0\n",
    "    classified[to_classify[1] == 1.0] = 0\n",
    "    classified[to_classify[1] == -1.0] = 0\n",
    "    \n",
    "    # set pixels within cloud cover to zero\n",
    "    classified[c == 1] = 0\n",
    "    \n",
    "    name = str(idx + 1) + '.tif'\n",
    "    \n",
    "    # save current annual change to disk\n",
    "    write(classified, str(DIRS.proc / name), crs=WGS84, driver='GTiff',\n",
    "          transform=transform, compress='lzw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div class=\"bk-root\">\n",
       "    <div class=\"bk-plotdiv\" id=\"f27670c0-a137-493d-a08a-3612417f880e\"></div>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "(function(root) {\n",
       "  function embed_document(root) {\n",
       "    \n",
       "  var docs_json = {\"d0b3b96f-506d-4e79-b589-4f21f0a5826e\":{\"roots\":{\"references\":[{\"attributes\":{\"dimension\":1,\"plot\":{\"id\":\"5a437ad2-82f8-4cd1-beac-2d7c2217cdf3\",\"subtype\":\"Figure\",\"type\":\"Plot\"},\"ticker\":{\"id\":\"bae015ae-e252-49a4-887d-2205a5715631\",\"type\":\"BasicTicker\"}},\"id\":\"9948d30c-ebf6-428f-8207-4779501bc077\",\"type\":\"Grid\"},{\"attributes\":{\"axis_label\":\"NDVI\",\"formatter\":{\"id\":\"9aa4190a-ec38-4ec4-8123-d0e397bda2d0\",\"type\":\"BasicTickFormatter\"},\"plot\":{\"id\":\"5a437ad2-82f8-4cd1-beac-2d7c2217cdf3\",\"subtype\":\"Figure\",\"type\":\"Plot\"},\"ticker\":{\"id\":\"d5f7a7c1-c3fe-420d-ba1d-a12aa4c98cd7\",\"type\":\"BasicTicker\"}},\"id\":\"db1b9784-01e8-4619-85ee-c32ee10ad19c\",\"type\":\"LinearAxis\"},{\"attributes\":{\"data_source\":{\"id\":\"3e451c5d-0deb-4e9d-bb7e-2bcea5464046\",\"type\":\"ColumnDataSource\"},\"glyph\":{\"id\":\"55dd7402-e6de-4d90-8012-711b2d8cd9ea\",\"type\":\"Line\"},\"hover_glyph\":null,\"muted_glyph\":null,\"nonselection_glyph\":{\"id\":\"fba22402-ac47-477b-9a9c-0d1f3d85b2d1\",\"type\":\"Line\"},\"selection_glyph\":null,\"view\":{\"id\":\"a7613de4-fc8f-43f2-ab6b-842a27ed5476\",\"type\":\"CDSView\"}},\"id\":\"b0453ff9-1ea1-43ce-ae18-7292e3819d74\",\"type\":\"GlyphRenderer\"},{\"attributes\":{},\"id\":\"76ac5ad9-b38b-4627-b369-aec32e161054\",\"type\":\"LinearScale\"},{\"attributes\":{},\"id\":\"c282aa8a-f519-47e2-af2f-03d4dc1906c8\",\"type\":\"WheelZoomTool\"},{\"attributes\":{\"callback\":null},\"id\":\"434e7884-be7c-405f-98e6-d0b8566cded9\",\"type\":\"DataRange1d\"},{\"attributes\":{\"callback\":null,\"column_names\":[\"y\",\"x\"],\"data\":{\"x\":{\"__ndarray__\":\"mpmZmZmZuT+amZmZmZnpPw==\",\"dtype\":\"float64\",\"shape\":[2]},\"y\":{\"__ndarray__\":\"etj4Zu86z7+MIKCVrnDUvw==\",\"dtype\":\"float64\",\"shape\":[2]}}},\"id\":\"3e451c5d-0deb-4e9d-bb7e-2bcea5464046\",\"type\":\"ColumnDataSource\"},{\"attributes\":{\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"}},\"id\":\"55dd7402-e6de-4d90-8012-711b2d8cd9ea\",\"type\":\"Line\"},{\"attributes\":{\"items\":[{\"id\":\"740e1dad-9d7d-4efe-bd4a-e0ec5dedba17\",\"type\":\"LegendItem\"}],\"plot\":{\"id\":\"5a437ad2-82f8-4cd1-beac-2d7c2217cdf3\",\"subtype\":\"Figure\",\"type\":\"Plot\"}},\"id\":\"0d34e5ab-d37e-450e-8eec-7800dfbb2c72\",\"type\":\"Legend\"},{\"attributes\":{\"plot\":{\"id\":\"5a437ad2-82f8-4cd1-beac-2d7c2217cdf3\",\"subtype\":\"Figure\",\"type\":\"Plot\"},\"ticker\":{\"id\":\"d5f7a7c1-c3fe-420d-ba1d-a12aa4c98cd7\",\"type\":\"BasicTicker\"}},\"id\":\"f05f9b08-8720-489f-b525-d37c675291ce\",\"type\":\"Grid\"},{\"attributes\":{},\"id\":\"bae015ae-e252-49a4-887d-2205a5715631\",\"type\":\"BasicTicker\"},{\"attributes\":{\"below\":[{\"id\":\"db1b9784-01e8-4619-85ee-c32ee10ad19c\",\"type\":\"LinearAxis\"}],\"left\":[{\"id\":\"65f566fa-23fa-41ab-9028-1aace90213c0\",\"type\":\"LinearAxis\"}],\"renderers\":[{\"id\":\"db1b9784-01e8-4619-85ee-c32ee10ad19c\",\"type\":\"LinearAxis\"},{\"id\":\"f05f9b08-8720-489f-b525-d37c675291ce\",\"type\":\"Grid\"},{\"id\":\"65f566fa-23fa-41ab-9028-1aace90213c0\",\"type\":\"LinearAxis\"},{\"id\":\"9948d30c-ebf6-428f-8207-4779501bc077\",\"type\":\"Grid\"},{\"id\":\"fccf108d-9524-4a4f-90a1-b81d7973cf81\",\"type\":\"BoxAnnotation\"},{\"id\":\"0d34e5ab-d37e-450e-8eec-7800dfbb2c72\",\"type\":\"Legend\"},{\"id\":\"c785608c-cf94-4a34-8b06-a4ee68466fa6\",\"type\":\"GlyphRenderer\"},{\"id\":\"b0453ff9-1ea1-43ce-ae18-7292e3819d74\",\"type\":\"GlyphRenderer\"}],\"title\":{\"id\":\"3a79f1aa-39dd-488d-9d71-b12038bc3491\",\"type\":\"Title\"},\"toolbar\":{\"id\":\"3e1f1508-7c34-40c6-b2ca-c6801c38925c\",\"type\":\"Toolbar\"},\"x_range\":{\"id\":\"434e7884-be7c-405f-98e6-d0b8566cded9\",\"type\":\"DataRange1d\"},\"x_scale\":{\"id\":\"b6c50259-0e3c-45d0-90b2-21965b8b9e56\",\"type\":\"LinearScale\"},\"y_range\":{\"id\":\"4ccc4a1c-3904-403c-889d-77696174e9f2\",\"type\":\"DataRange1d\"},\"y_scale\":{\"id\":\"76ac5ad9-b38b-4627-b369-aec32e161054\",\"type\":\"LinearScale\"}},\"id\":\"5a437ad2-82f8-4cd1-beac-2d7c2217cdf3\",\"subtype\":\"Figure\",\"type\":\"Plot\"},{\"attributes\":{},\"id\":\"bac71ca9-ee8d-4cdd-acee-c25647d0fe8d\",\"type\":\"SaveTool\"},{\"attributes\":{\"callback\":null},\"id\":\"4ccc4a1c-3904-403c-889d-77696174e9f2\",\"type\":\"DataRange1d\"},{\"attributes\":{\"label\":{\"field\":\"name\"},\"renderers\":[{\"id\":\"c785608c-cf94-4a34-8b06-a4ee68466fa6\",\"type\":\"GlyphRenderer\"}]},\"id\":\"740e1dad-9d7d-4efe-bd4a-e0ec5dedba17\",\"type\":\"LegendItem\"},{\"attributes\":{\"bottom_units\":\"screen\",\"fill_alpha\":{\"value\":0.5},\"fill_color\":{\"value\":\"lightgrey\"},\"left_units\":\"screen\",\"level\":\"overlay\",\"line_alpha\":{\"value\":1.0},\"line_color\":{\"value\":\"black\"},\"line_dash\":[4,4],\"line_width\":{\"value\":2},\"plot\":null,\"render_mode\":\"css\",\"right_units\":\"screen\",\"top_units\":\"screen\"},\"id\":\"fccf108d-9524-4a4f-90a1-b81d7973cf81\",\"type\":\"BoxAnnotation\"},{\"attributes\":{},\"id\":\"b6c50259-0e3c-45d0-90b2-21965b8b9e56\",\"type\":\"LinearScale\"},{\"attributes\":{\"fill_alpha\":{\"value\":0.1},\"fill_color\":{\"value\":\"#1f77b4\"},\"line_alpha\":{\"value\":0.1},\"line_color\":{\"value\":\"#1f77b4\"},\"x\":{\"field\":\"ndvi\"},\"y\":{\"field\":\"bi\"}},\"id\":\"a0563927-052e-4862-886d-614ad2ad36cf\",\"type\":\"Circle\"},{\"attributes\":{\"fill_color\":{\"field\":\"color\"},\"line_color\":{\"field\":\"color\"},\"x\":{\"field\":\"ndvi\"},\"y\":{\"field\":\"bi\"}},\"id\":\"b14aed54-1352-43d6-bc3f-203491a37bd9\",\"type\":\"Circle\"},{\"attributes\":{\"data_source\":{\"id\":\"0d4a44af-13e2-4b73-89a9-31abd3df6b9e\",\"type\":\"ColumnDataSource\"},\"glyph\":{\"id\":\"b14aed54-1352-43d6-bc3f-203491a37bd9\",\"type\":\"Circle\"},\"hover_glyph\":null,\"muted_glyph\":null,\"nonselection_glyph\":{\"id\":\"a0563927-052e-4862-886d-614ad2ad36cf\",\"type\":\"Circle\"},\"selection_glyph\":null,\"view\":{\"id\":\"05195ccb-2b58-41d1-adaf-84fafc373fc5\",\"type\":\"CDSView\"}},\"id\":\"c785608c-cf94-4a34-8b06-a4ee68466fa6\",\"type\":\"GlyphRenderer\"},{\"attributes\":{},\"id\":\"fbd5d452-0a18-4416-8de4-be33a8f60779\",\"type\":\"ResetTool\"},{\"attributes\":{\"line_alpha\":0.1,\"line_color\":\"#1f77b4\",\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"}},\"id\":\"fba22402-ac47-477b-9a9c-0d1f3d85b2d1\",\"type\":\"Line\"},{\"attributes\":{},\"id\":\"c803a589-569b-4a10-b3ad-19a1618e4207\",\"type\":\"HelpTool\"},{\"attributes\":{},\"id\":\"4c6b9199-2e53-4551-83e2-946f52224a49\",\"type\":\"PanTool\"},{\"attributes\":{},\"id\":\"d5f7a7c1-c3fe-420d-ba1d-a12aa4c98cd7\",\"type\":\"BasicTicker\"},{\"attributes\":{\"callback\":null,\"column_names\":[\"ndvi\",\"bi\",\"index\",\"name\",\"label\",\"color\"],\"data\":{\"bi\":{\"__ndarray__\":\"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\",\"dtype\":\"float64\",\"shape\":[334]},\"color\":[\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"green\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\"],\"index\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333],\"label\":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1],\"name\":[\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Forest\",\"Mine\",\"Mine\",\"Mine\",\"Mine\",\"Mine\",\"Mine\",\"Mine\",\"Mine\",\"Mine\",\"Mine\",\"Mine\",\"Mine\",\"Mine\",\"Mine\",\"Mine\",\"Mine\",\"Mine\",\"Mine\",\"Mine\",\"Mine\",\"Mine\",\"Mine\",\"Mine\",\"Mine\",\"Mine\",\"Mine\",\"Mine\",\"Mine\",\"Mine\",\"Mine\",\"Mine\",\"Mine\",\"Mine\",\"Mine\",\"Mine\",\"Mine\",\"Mine\",\"Mine\",\"Mine\",\"Mine\",\"Mine\",\"Mine\",\"Mine\",\"Mine\",\"Mine\",\"Mine\",\"Mine\",\"Mine\",\"Mine\",\"Mine\",\"Mine\",\"Mine\",\"Mine\",\"Mine\",\"Mine\",\"Mine\",\"Mine\",\"Mine\",\"Mine\",\"Mine\",\"Mine\",\"Mine\",\"Mine\",\"Mine\",\"Mine\",\"Mine\",\"Mine\",\"Mine\",\"Mine\",\"Mine\",\"Mine\",\"Mine\",\"Mine\",\"Mine\",\"Mine\",\"Mine\",\"Mine\",\"Mine\",\"Mine\",\"Mine\",\"Mine\",\"Mine\",\"Mine\",\"Mine\",\"Mine\",\"Mine\",\"Mine\",\"Mine\",\"Mine\",\"Mine\",\"Mine\",\"Mine\",\"Mine\"],\"ndvi\":{\"__ndarray__\":\"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\",\"dtype\":\"float64\",\"shape\":[334]}}},\"id\":\"0d4a44af-13e2-4b73-89a9-31abd3df6b9e\",\"type\":\"ColumnDataSource\"},{\"attributes\":{\"active_drag\":\"auto\",\"active_inspect\":\"auto\",\"active_scroll\":\"auto\",\"active_tap\":\"auto\",\"tools\":[{\"id\":\"4c6b9199-2e53-4551-83e2-946f52224a49\",\"type\":\"PanTool\"},{\"id\":\"c282aa8a-f519-47e2-af2f-03d4dc1906c8\",\"type\":\"WheelZoomTool\"},{\"id\":\"c7f5fc5b-8a02-4f79-9df0-c73b56c5b61d\",\"type\":\"BoxZoomTool\"},{\"id\":\"bac71ca9-ee8d-4cdd-acee-c25647d0fe8d\",\"type\":\"SaveTool\"},{\"id\":\"fbd5d452-0a18-4416-8de4-be33a8f60779\",\"type\":\"ResetTool\"},{\"id\":\"c803a589-569b-4a10-b3ad-19a1618e4207\",\"type\":\"HelpTool\"}]},\"id\":\"3e1f1508-7c34-40c6-b2ca-c6801c38925c\",\"type\":\"Toolbar\"},{\"attributes\":{\"overlay\":{\"id\":\"fccf108d-9524-4a4f-90a1-b81d7973cf81\",\"type\":\"BoxAnnotation\"}},\"id\":\"c7f5fc5b-8a02-4f79-9df0-c73b56c5b61d\",\"type\":\"BoxZoomTool\"},{\"attributes\":{},\"id\":\"9aa4190a-ec38-4ec4-8123-d0e397bda2d0\",\"type\":\"BasicTickFormatter\"},{\"attributes\":{\"source\":{\"id\":\"3e451c5d-0deb-4e9d-bb7e-2bcea5464046\",\"type\":\"ColumnDataSource\"}},\"id\":\"a7613de4-fc8f-43f2-ab6b-842a27ed5476\",\"type\":\"CDSView\"},{\"attributes\":{\"plot\":null,\"text\":\"Spectral pattern of mines versus forest\"},\"id\":\"3a79f1aa-39dd-488d-9d71-b12038bc3491\",\"type\":\"Title\"},{\"attributes\":{\"axis_label\":\"NDBI\",\"formatter\":{\"id\":\"0005aec5-fa3c-4e03-bed1-e2f80f4d9509\",\"type\":\"BasicTickFormatter\"},\"plot\":{\"id\":\"5a437ad2-82f8-4cd1-beac-2d7c2217cdf3\",\"subtype\":\"Figure\",\"type\":\"Plot\"},\"ticker\":{\"id\":\"bae015ae-e252-49a4-887d-2205a5715631\",\"type\":\"BasicTicker\"}},\"id\":\"65f566fa-23fa-41ab-9028-1aace90213c0\",\"type\":\"LinearAxis\"},{\"attributes\":{},\"id\":\"0005aec5-fa3c-4e03-bed1-e2f80f4d9509\",\"type\":\"BasicTickFormatter\"},{\"attributes\":{\"source\":{\"id\":\"0d4a44af-13e2-4b73-89a9-31abd3df6b9e\",\"type\":\"ColumnDataSource\"}},\"id\":\"05195ccb-2b58-41d1-adaf-84fafc373fc5\",\"type\":\"CDSView\"}],\"root_ids\":[\"5a437ad2-82f8-4cd1-beac-2d7c2217cdf3\"]},\"title\":\"Bokeh Application\",\"version\":\"0.12.13\"}};\n",
       "  var render_items = [{\"docid\":\"d0b3b96f-506d-4e79-b589-4f21f0a5826e\",\"elementid\":\"f27670c0-a137-493d-a08a-3612417f880e\",\"modelid\":\"5a437ad2-82f8-4cd1-beac-2d7c2217cdf3\"}];\n",
       "  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\n",
       "\n",
       "  }\n",
       "  if (root.Bokeh !== undefined) {\n",
       "    embed_document(root);\n",
       "  } else {\n",
       "    var attempts = 0;\n",
       "    var timer = setInterval(function(root) {\n",
       "      if (root.Bokeh !== undefined) {\n",
       "        embed_document(root);\n",
       "        clearInterval(timer);\n",
       "      }\n",
       "      attempts++;\n",
       "      if (attempts > 100) {\n",
       "        console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\")\n",
       "        clearInterval(timer);\n",
       "      }\n",
       "    }, 10, root)\n",
       "  }\n",
       "})(window);"
      ],
      "application/vnd.bokehjs_exec.v0+json": ""
     },
     "metadata": {
      "application/vnd.bokehjs_exec.v0+json": {
       "id": "5a437ad2-82f8-4cd1-beac-2d7c2217cdf3"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot classifed samples and the projected linear discriminant \n",
    "\n",
    "data = gpd.read_file(str(DIRS.prep / 'mines.shp'))\n",
    "data['color'] = data.apply(lambda row: 'green' if row['label'] == 0 else 'red',\n",
    "                           axis=1)\n",
    "data['name'] = data.apply(lambda row: 'Forest' if row['label'] == 0 else 'Mine',\n",
    "                           axis=1)\n",
    "data.drop('geometry', axis=1, inplace=True)\n",
    "\n",
    "w = lda.coef_[0]\n",
    "a = -w[0] / w[1]\n",
    "xx = np.array([0.1,0.8])\n",
    "yy = a * xx - (lda.intercept_[0]) / w[1]\n",
    "\n",
    "src = ColumnDataSource(data)\n",
    "\n",
    "scatter = figure(title='Spectral pattern of mines versus forest')\n",
    "\n",
    "scatter.circle(x='ndvi', y='bi', color='color', legend='name', source=src)\n",
    "scatter.line(x=xx, y=yy, color='black')\n",
    "\n",
    "scatter.xaxis.axis_label = \"NDVI\"\n",
    "scatter.yaxis.axis_label = \"NDBI\"\n",
    "\n",
    "show(scatter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results and discussion\n",
    "#### Tree cover 2000\n",
    "A total of 500 pixel samples was drawn from the reference image and subsequently labeled as tree cover and no tree cover through visual interpretation. A total of 259 samples fall into the class without a tree cover and the remaining 241 samples show a tree cover. The Random Forest classifier was trained with 364 samples from the labeled dataset and an accuracy assessment was determined with the remaining 136 samples. Eventually, the accuracy assessment is used to create the following confusion matrix.\n",
    "\n",
    " | Condition positive (Tree cover) | Condition negative (other) \n",
    ":--- | :---: | :---: \n",
    "**Prediction positive (Tree cover)** | 65 | 4\n",
    "**Prediction negative (other)** | 5 | 62\n",
    "\n",
    "**Table 2:** Confusion matrix for accuracy assessment.\n",
    "\n",
    "The overall accuracy of the tree cover classification is approximately 93 %. [**Figure 3**](img/treecover.png) shows the estimated tree cover for the study area at 2000. Approximately 5222.23 km$^2$ are covered by trees, respectively 49 % of the study area.  \n",
    "\n",
    "Due to the relative small sample size and absence of any ground truth data the results of the tree cover classification are quite promising. The processing pipeline in this study serves just as a prototype and a real world study should use a bigger sample size as the literature suggest. Furthermore, the visual labeling should be supported by ground truth data like forest inventories or else. Additionally the result should be compared with other tree cover estimates to valid the estimates. Hansen et al. study provides a global tree cover layer which can be used for this validation.\n",
    "\n",
    "![Treecover2000](img/treecover.png)\n",
    "**Figure 2:** Estimated tree cover for AOI at 2000."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Amber mining sites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**Figure 3**](mining.png) and [**Figure 4**](img/detail.png) show the annual land cover change from forest to amber mines. Approximately 7 % of the initial tree cover is affected by land cover changes. In detail: 2001 61.83 km$^2$, 2002 49.63 km$^2$, 2005 44.73 km$^2$, 2007 40.35 km$^2$, 2008 66.49 km$^2$, 2010 100.80 km$^2$ and 2012 48.59 km$^2$.  \n",
    "This values are in-line with the increasing demand for amber since 2008 mentioned in the introduction. However, it should be mentioned that the sum is greater than the estimate from Hnatushenko et al. with 500 ha \\cite{Hnatushenko2017}. The estimates of this study exceeds the prediction by Hnatushenko et al. because the chosen approach is not selective enough. Solely to relay on the NDVI and NDBI for a amber mine detection rejects the presence of other land cover changes, which transform tree cover to large areas of bare land. For instance a transformation to cultivated land or a large clearcut should show a comparable spectral pattern. Furthermore, the study relayed only on guessed occurrences of amber mines on satellite images. Without appropriate field data it is hard to develop a more specific approach. Finally, it should be mentioned that this approach is a prototype, and with some tweaks  the processing pipeline as well as input data it could be an more mature method to estimate the tree cover affected by amber mining."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Mining](img/mining.png)\n",
    "**Figure 3:** Annual changes in tree cover through amber mining disturbances.\n",
    "![Detail](img/detail.png)\n",
    "**Figure 4:** Annual changes in tree cover zoom to right to corner with the highest density of changes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "[<a id=\"cit-Piechal2017\" href=\"#call-Piechal2017\">1</a>] Piechal Tomasz, ``_The Amber Rush in Ukraine_'', OSW Commentary: Centre for Eastern Studies, vol. , number 241, pp. , May 2017.\n",
    "\n",
    "[<a id=\"cit-Hoffman2017\" href=\"#call-Hoffman2017\">2</a>] Brendan Hoffman, ``_Image_'', January 2017.  [online](https://news.nationalgeographic.com/2017/01/illegal-amber-mining-ukraine/)\n",
    "\n",
    "[<a id=\"cit-Hnatushenko2017\" href=\"#call-Hnatushenko2017\">3</a>] V. V., K. D., V. V. <em>et al.</em>, ``_Satellite monitoring of consequences of illegal extraction of amber in Ukraine_'', Scientific bulletin of NMU, vol. , number 2, pp. 99--105,  2017.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autocomplete": true,
   "bibliofile": "lit.bib",
   "cite_by": "number",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
